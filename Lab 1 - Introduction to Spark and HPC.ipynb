{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Introduction to (Py)Spark and (Sheffield)HPC\n",
    "\n",
    "[COM6012 Scalable Machine Learning **2021**](https://github.com/haipinglu/ScalableML) by [Haiping Lu](http://staffwww.dcs.shef.ac.uk/people/H.Lu/) at The University of Sheffield\n",
    "\n",
    "# NOT Ready yet. Still in Preparation\n",
    "\n",
    "## Study schedule\n",
    "\n",
    "* Task 1: To finish by Wednesday. **Critical**\n",
    "* Task 2: To finish by Wednesday. **Critical**\n",
    "* Task 3: To finish by Thursday. **Essential**\n",
    "* Task 4: To finish by Thursday. **Essential**\n",
    "* Task 5: To finish before the next Monday. ***Exercise***\n",
    "* Task 6: To explore further. *Optional*\n",
    "\n",
    "**Suggested reading**: \n",
    "* Chapters 2 to 4 of [PySpark tutorial](https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf) (several sections in Chapter 3 can be safely skipped)\n",
    "* [Spark Quick Start](https://spark.apache.org/docs/3.0.1/quick-start.html) (Choose **Python** rather than the default *scala*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Note - Please read before proceeding</span>**:\n",
    "- HPC nodes are **shared** resources (**like buses/trains**) relying on considerate usage of every user. When requesting resources, if you ask for too much (e.g. 50 cores), it will take a long time to get allocated, particularly during \"*rush hours*\" (e.g. close to deadlines) and once allocated, it will leave much less for the others. If everybody is asking for too much, the system won't work and everyone suffers.\n",
    "- We have five nodes (each with 40 cores, 768GB RAM) reserved for this module. You can specify `-P rse-com6012` (e.g. after `qrshx`) to get access. However, these nodes are not always more available, e.g. if all of us are using it. There are **100+** regular nodes, many of which may be idle.\n",
    "- Please follow **all steps (step by step without skipping)** unless you are very confident in handling problems by yourself. \n",
    "- Please try your best to follow the **study schedule** above to finish the tasks on time. If you start early/on time, you will find your problems early so that you can make good use of the online sessions to get help from the instructors and teaching assistants to fix your problems early, rather than getting panic close to an assessment deadline. Based on our experience from the past four years, rushing towards an assessment deadline in this module is likely to make you fall, sometimes painfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to HPC and Install Spark\n",
    "\n",
    "You **must** first connect to the [university's VPN](https://www.sheffield.ac.uk/it-services/vpn) unless you are on the campus network, which is unlikely during the lockdown.\n",
    "\n",
    "### 1.1 Connect to ShARC HPC via SSH\n",
    "\n",
    "Follow the [official instruction](https://docs.hpc.shef.ac.uk/en/latest/hpc/index.html) from our university. I have get your HPC account created already (first step done). Use your university **username** such as `abc18de` and the associated password to log in. \n",
    "\n",
    "If you have problem logging in, email me 1) your username; 2) a connection status of your VPN, 3) a screen capture of the error message (**never send me your password**). If you can log in but encounter HPC-related problem, you can email ` hpc@sheffield.ac.uk` for help.\n",
    "\n",
    "Following the [official instructins](https://docs.hpc.shef.ac.uk/en/latest/hpc/connecting.html) for [Windows](https://docs.hpc.shef.ac.uk/en/latest/hpc/connecting.html#ssh-client-software-on-windows) or [Mac OS/X and Linux](https://docs.hpc.shef.ac.uk/en/latest/hpc/connecting.html#ssh-client-software-on-mac-os-x-and-linux) to open a terminal and connect to sharc via SSH by\n",
    "\n",
    "`ssh -X $USER@$sharc.shef.ac.uk`\n",
    "\n",
    "You need to replace `$USER` with your username, let's assume it is `abc1de`. If successful, you should see \n",
    "\n",
    "`[abc1de@sharc-login1 ~]$`\n",
    "\n",
    "`abc1de` should be your username. \n",
    "\n",
    "**MobaXterm tips**\n",
    "\n",
    "- You can save the host, username (and password if your computer is secure) as a **Session** if you want to save time in future.\n",
    "- Shift + Insert for copy (after text selection), not Ctrl + C. Ctrl + V works as paste (to confirm on new PC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set up the environment and install PySpark\n",
    "\n",
    "#### Start an interactive session\n",
    "\n",
    "Type `qrshx` for a *regular* node **or** `qrshx -P rse-com6012` for a com6012-reserved node\n",
    "\n",
    "#### Load Java and conda\n",
    "\n",
    "`module load apps/java/jdk1.8.0_102/binary`\n",
    "\n",
    "`module load apps/python/conda`\n",
    "\n",
    "#### Create a virtual environment called `myspark`\n",
    "\n",
    "`conda create -n myspark python=3.6`\n",
    "\n",
    "When you are asked whether to proceed, say `y`\n",
    "\n",
    "#### Activate the environment\n",
    "\n",
    "`source activate myspark`\n",
    "\n",
    "You **must** see `(myspark) [abc1de@sharc-nodeXXX ~]$`, i.e. **(myspark)** in front, before proceeding. Otherwise, you did not get the proper environment. Check the above steps. \n",
    "\n",
    "#### Install pyspark 3.0.1 using `pip`\n",
    "\n",
    "`pip install pyspark==3.0.1`\n",
    "\n",
    "When you are asked whether to proceed, say `y`\n",
    "\n",
    "#### Run spark\n",
    "\n",
    "`pyspark`\n",
    "\n",
    "You should see spark version **3.0.1** displayed. Quit pyspark shell by `Ctrl + D`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Get more familiar with the HPC\n",
    "\n",
    "**Terminal/command line**: learn the [basic use of the command line](https://github.com/mikecroucher/Intro_to_HPC/blob/gh-pages/terminal_tutorial.md) in Linux, e.g. use `pwd` to find out your **current directory**.\n",
    "\n",
    "**Transfer files**: learn how to [transfer files to/from ShARC HPC](https://www.sheffield.ac.uk/it-services/research/hpc/using/access).\n",
    "\n",
    "**Line ending warning**: if you are using Windows, you should be aware that [line endings differ between Windows and Linux](https://stackoverflow.com/questions/426397/do-line-endings-differ-between-windows-and-linux). If you edit a shell script (below) in Windows, make sure that you use a Unix/Linux compatible editor or do the conversion before using it on HPC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 *Optional: Install PySpark on your own machine*  \n",
    "\n",
    "This module focuses on the HPC terminal. Labs are in the format of Jupyter Notebooks but you should use the HPC terminal to complete the labs. ALL assessments use the HPC terminal.\n",
    "\n",
    "Installation of PySpark on your own machine is more complicated than installing a regualr python library because it depends on Java (i.e. not pure python). Four basic steps are\n",
    "\n",
    "- Intall **Java 8**, i.e. java version *1.8.xxx* (not Java 11, or 1.11) via [**Java JRE**](https://www.oracle.com/java/technologies/javase-jre8-downloads.html). Most instructions online ask you to install *Java SDK*, which is heavy but unnecessary. Actually you only need to install [**Java JRE**](https://www.oracle.com/java/technologies/javase-jre8-downloads.html), which is light and sufficient.\n",
    "- Install Python **3.6+** (if not yet)\n",
    "- Install PySpark **3.0.1** with **Hadoop 2.7**\n",
    "- Set up the proper environments (see references below)\n",
    "\n",
    "As far as I know, it is not necessary to install *Scala*.\n",
    "\n",
    "Different OS (Windows/Linux/Mac) may have different problems. We provide some references below if you wish to try but it is *not required* and we can provide only very limited support on this task (i.e. we may not be able to solve all problems that you may encounter).\n",
    "\n",
    "If you do want to install PySpark and run Jupyter Notebooks on your own machine, you need to complete the steps above with reference to the instructions below for your OS (Windows/Linux/Mac)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References (use with caution, not necessarily up to date or the best)\n",
    "\n",
    "If you follow the steps in these references, be aware that they are not up to date so you should install the correct versions: **Java 1.8**, Python **3.6+**, PySpark **3.0.1** with **Hadoop 2.7**. *Scala* is optional.\n",
    "\n",
    "* Windows: 1) [Install Spark on Windows (PySpark)](https://medium.com/@GalarnykMichael/install-spark-on-windows-pyspark-4498a5d8d66c) (with video) 2) [How to install Spark on Windows in 5 steps](https://medium.com/@dvainrub/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3) **Note:** The following may be needed. Go to your System Environment Variables and add PYTHONPATH to it with the following value: `%SPARK_HOME%\\python;%SPARK_HOME%\\python\\lib\\py4j-<version>-src.zip;%PYTHONPATH%`, just check what py4j version you have in your `spark/python/lib` folder ([source](https://stackoverflow.com/questions/53161939/pyspark-error-does-not-exist-in-the-jvm-error-when-initializing-sparkcontext?noredirect=1&lq=1)).\n",
    "\n",
    "* Linux: 1) [Install PySpark on Ubuntu](https://medium.com/@GalarnykMichael/install-spark-on-ubuntu-pyspark-231c45677de0) (with video); 2)[Installing PySpark with JAVA 8 on ubuntu 18.04](https://towardsdatascience.com/installing-pyspark-with-java-8-on-ubuntu-18-04-6a9dea915b5b)\n",
    "\n",
    "* Mac: 1) [Install Spark on Mac (PySpark)](https://medium.com/@GalarnykMichael/install-spark-on-mac-pyspark-453f395f240b) (with video); 2) [Install Spark/PySpark on Mac](https://medium.com/@yajieli/installing-spark-pyspark-on-mac-and-fix-of-some-common-errors-355a9050f735)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install PySpark on Windows\n",
    "\n",
    "Here we provide more detailed instructions only for Windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Spark\n",
    "\n",
    "### On HPC: get a node and activate myspark\n",
    "\n",
    "- Get a node via `qrshx` or `qrshx -P rse-com6012`.\n",
    "- Activate the environment by\n",
    "\n",
    "  `module load apps/java/jdk1.8.0_102/binary`\n",
    "\n",
    "  `module load apps/python/conda`\n",
    "\n",
    "  `source activate myspark`\n",
    "  or alternatively, put `HPC/myspark.sh` under your root directory (see above on how to transfer files) and do\n",
    "  \n",
    "  `source myspark.sh` will run the three commands in sequence. You could modify it further to suit yourself better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive (HPC or local machine)\n",
    "\n",
    "#### If running notebook of spark on your local machine\n",
    "**Note** If `import pyspark` reports error, you may try `pip install findspark`, `import findspark`, \n",
    "`findspark.init()`, and then `import pyspark` should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Spark Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If running spark in a shell on either HPC or your local machine, `spark` (SparkSession) and `sc` (SparkContext) is automatically created.\n",
    "\n",
    "Run pyspark (optionally, specify to use multiple cores)\n",
    "\n",
    "`pyspark` or `pyspark --master local[2]` with two cores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your SparkSession and SparkContext object (you will see different output if running in a shell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://143.167.111.234:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>COM6012 Spark Intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e114963d68>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and check sc (SparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://143.167.111.234:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>COM6012 Spark Intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=COM6012 Spark Intro>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4])\n",
    "nums.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Log Mining with Spark - Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example deals with **Semi-Structured** data in a text file. \n",
    "\n",
    "Firstly, you need to **make sure the file is in the proper directory and change the file path if necessary**, on either HPC or local machine.\n",
    "\n",
    "**If running on HPC, you need to transfer files there.** Here is how to [**transfer files to HPC**](https://www.sheffield.ac.uk/cics/research/hpc/using/access). Please **click** and follow the instructions unless you are already familiar with it.\n",
    "\n",
    "### GUI-based file transfer\n",
    "\n",
    "* [**MobaXterm**](https://mobaxterm.mobatek.net/) is recommended for **Windows**\n",
    "* [**Cyberduck**](https://en.wikipedia.org/wiki/Cyberduck) or [**FileZilla**](https://en.wikipedia.org/wiki/FileZilla) is recommended for **Mac**\n",
    "* **FileZilla** is recommended for **Linux (e.g., Ubuntu)**\n",
    "\n",
    "For example, in MobaXterm (for Windows), you just need to **Drag your file or folder to the left directory pane of MobaXterm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFile=spark.read.text(\"Data/NASA_Aug95_100.txt\")\n",
    "logFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] \"GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0\" 200 1839')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How many accesses are from Japan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hostsJapan = logFile.filter(logFile.value.contains(\".jp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether you are getting what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                         |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "|kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:17 -0400] \"GET / HTTP/1.0\" 200 7280                         |\n",
      "|kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:18 -0400] \"GET /images/ksclogo-medium.gif HTTP/1.0\" 200 5866|\n",
      "|kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:21 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0   |\n",
      "|kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:21 -0400] \"GET /images/MOSAIC-logosmall.gif HTTP/1.0\" 304 0 |\n",
      "|kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:22 -0400] \"GET /images/USA-logosmall.gif HTTP/1.0\" 304 0    |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hostsJapan.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hostsJapan.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-contained Application\n",
    "\n",
    "To run a self-contained application, you need to **exit your shell, by `Ctrl+D` first**.\n",
    "\n",
    "Create a file `LogMining100.py`\n",
    "\n",
    "~~~~\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Spark Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "logFile=spark.read.text(\"Data/NASA_Aug95_100.txt\")\n",
    "hostsJapan = logFile.filter(logFile.value.contains(\".jp\")).count()\n",
    "\n",
    "print(\"\\n\\nHello Spark: There are %i hosts from Japan.\\n\\n\" % (hostsJapan))\n",
    "\n",
    "spark.stop()\n",
    "~~~~\n",
    "\n",
    "\n",
    "Then run it with `spark-submit Code/LogMining100.py`  **Note: You need exit your shell, by `Ctrl+D` first**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Big Data Log Mining with Spark \n",
    "\n",
    "**Data**: Download the August data in gzip (NASA_access_log_Aug95.gz) from [NASA HTTP server access log](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html) and put into your `Data` folder. `NASA_Aug95_100.txt` above is the first 100 lines of the August data.\n",
    "\n",
    "**Question**: How many accesses are from Japan and UK respectively?\n",
    "\n",
    "Create a file `LogMiningBig.py`\n",
    "\n",
    "~~~~\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Spark Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "logFile=spark.read.text(\"../Data/NASA_access_log_Aug95.gz\").cache()\n",
    "\n",
    "hostsJapan = logFile.filter(logFile.value.contains(\".jp\")).count()\n",
    "hostsUK = logFile.filter(logFile.value.contains(\".uk\")).count()\n",
    "\n",
    "print(\"\\n\\nHello Spark: There are %i hosts from UK.\\n\" % (hostsUK))\n",
    "print(\"Hello Spark: There are %i hosts from Japan.\\n\\n\" % (hostsJapan))\n",
    "\n",
    "spark.stop()\n",
    "~~~~\n",
    "**Spark can read gzip file directly. You do not need to unzip it to a big file.**\n",
    "\n",
    "**Note the use of cache() above**\n",
    "\n",
    "### Run a program in batch mode\n",
    "\n",
    "[How to submi batch jobs to ShARC](https://www.sheffield.ac.uk/cics/research/hpc/sharc/batch) **The more resources you request, the longer you need to queue**\n",
    "\n",
    "Interactive mode will be good for learning, exploring and debugging, with smaller data. For big data, it will be more convenient to use batch processing. You submit the job to the node to join a queue. Once allocated, your job will run, with output properly recorded. This is done via a shell script. **Warning: Do not create such a file under WINDOWS.**\n",
    "\n",
    "Create a file `Lab1_SubmitBatch.sh`\n",
    "\n",
    "~~~~\n",
    "#!/bin/bash\n",
    "#$ -l h_rt=2:00:00  #time needed\n",
    "#$ -pe smp 2 #number of cores\n",
    "#$ -l rmem=4G #number of memery\n",
    "#$ -o COM6012_Lab1.output #This is where your output and errors are logged.\n",
    "#$ -j y # normal and error outputs into a single file (the file above)\n",
    "#$ -M youremail@shef.ac.uk #Notify you by email, remove this line if you don't like\n",
    "#$ -m ea #Email you when it finished or aborted\n",
    "#$ -cwd # Run job from current directory\n",
    "\n",
    "module load apps/java/jdk1.8.0_102/binary\n",
    "\n",
    "module load apps/python/conda\n",
    "\n",
    "source activate myspark\n",
    "\n",
    "spark-submit ../Code/LogMiningBig.py\n",
    "~~~~\n",
    "\n",
    "* Get necessary files on your ShARC.\n",
    "* Start a session with command `qrshx`\n",
    "* Under appopriate directory (`HPC`) submit yur job via the `qsub` comand\n",
    "\n",
    "`qsub Lab1_SubmitBatch.sh`\n",
    "\n",
    "Check the status of your quening/running job(s) `qstat` (jobs not shown are finished already).\n",
    "\n",
    "Check your output file, which is **`COM6012_Lab1.output`** specified with option **`-o`** above. You can change it to a name you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exercises\n",
    "\n",
    "## 6. Additional ideas to explore\n",
    "\n",
    "### More mining questions (completing three or more questions is considered as completion of this exercise):\n",
    "\n",
    "#### Easier questions\n",
    "* How many requests in total?\n",
    "* How many requests on a particular day (e.g., 15th August)?\n",
    "* How many 404 (page not found) errors in total?\n",
    "* How many 404 (page not found) errors on a particular day (e.g., 15th August)?\n",
    "* How many requests from a particular host (e.g.,uplherc.up.com)?\n",
    "* Any other question that you are interested in.\n",
    "\n",
    "#### More challenging questions that will become easier to answer in Session 2 (optional for Session 1)\n",
    "* How many **unique** hosts on a particular day (e.g., 15th August)?\n",
    "* How many **unique** hosts in total (i.e., in August 1995)?\n",
    "* Which host is the most frequent visitor?\n",
    "* How many different types of return codes?\n",
    "* How many requests per day on average?\n",
    "* How many requests per post on average?\n",
    "* Any other question that you are interested in.\n",
    "\n",
    "### The effects of caching (recommended)\n",
    "* **Compare** the time taken to complete your jobs **with and without** `cache()`.\n",
    "\n",
    "# Acknowledgements\n",
    "\n",
    "Many thanks to Twin, Will, Mike, Vamsi for their kind help and all those kind contributors of open resources.\n",
    "\n",
    "The log mining problem is adapted from [UC Berkeley cs105x L3](https://www.edx.org/course/introduction-apache-spark-uc-berkeleyx-cs105x)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
